{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Green'>Sarcastic Comment Classification </font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Brown'>Author - Ishant Gupta</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>Observation</font> \n",
    "#### NB_Classifier = Using stopwords there was a decline in accuracy of 1%\n",
    "#### Random_Forest = Using stopwords and n_grams=(1,1) accuracy improved from 65% to 75%\n",
    "#### XGB_Classifier = Tuning the paramters using Grid Search improved the accuracy to 99.18%\n",
    "#### SVM = Tunning the paramters using Grid Search improved the accuracy from 53% to 74%\n",
    "#### CNN_Yon_Kim = A slow learning rate decrease the loss and improves the accuracy to 74.33% in 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfid = TfidfVectorizer()\n",
    "vect = CountVectorizer()\n",
    "vect_stopwords = CountVectorizer(stop_words='english')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.manifold import TSNE\n",
    "NB = MultinomialNB()\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    return text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    tweets = load_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_emoji_dictionary():\n",
    "    emojis = load_file(emoji_filename)\n",
    "    emoji_dict = {}\n",
    "    for line in emojis:\n",
    "        line = line.split(\" \", 1)\n",
    "        emoji = line[0]\n",
    "        description = line[1]\n",
    "        emoji_dict[emoji] = description\n",
    "    return emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_clean(tweets):\n",
    "        file = open(filename, 'r')\n",
    "        text = file.read()\n",
    "        text = text.split('\\n')\n",
    "        strict_tweets = []\n",
    "        emoji_dict = get_emoji_dictionary()\n",
    "        for tweet in text:\n",
    "            strict_tweet = []\n",
    "            for word in tweet.split():\n",
    "                if '#' in word:\n",
    "                    continue\n",
    "                if '@' in word:\n",
    "                    continue\n",
    "                if 'http' in word:\n",
    "                    continue\n",
    "                if check_if_emoji(word, emoji_dict):\n",
    "                    continue\n",
    "                strict_tweet.append(word)\n",
    "            strict_tweets.append(' '.join(strict_tweet))\n",
    "        return strict_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_emoji(word, emoji_dict):\n",
    "    emojis = list(word)\n",
    "    for em in emojis:\n",
    "        if em in emoji_dict.keys() or em in emoji.UNICODE_EMOJI:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDIT FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_filename = 'emoji_list.txt'\n",
    "filename = 'Train_v1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "with open(filename,'r') as f:\n",
    "    text = f.read()\n",
    "    l = text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_tweets = strict_clean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoflist = []\n",
    "for i in strict_tweets:\n",
    "    listoflist.append((i.split(' ',2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(listoflist,columns=['trainsen','labels','tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39781"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels='trainsen',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21292\n",
       "1    18488\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df['labels'].value_counts()\n",
    "counts = pd.DataFrame({'labels':count.index, 'count':count.values})\n",
    "#print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAHHCAYAAAD58fFKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3XmU1eV9+PHPDDADgwz7NhFQUHAnrhNcidAgUoMxbYilETR1QYhJNUpJqxjPaSC41aqxpidqc1KjsTXaqMEIQjWKohZUohLBEYyyqMjqwjLP74/85sbrgIDhGWaG1+ucOYf5fp97v88zX+be97nL3JKUUgoAgAxKd/cEAIDmS2gAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAI3G66+/HiUlJXHNNdfs7qkAu4jQgD1UTU1NTJgwIfr37x8VFRVRUVERBx10UIwfPz5eeOGF3T09oJloubsnADS8Bx54IEaNGhUtW7aM0aNHx8CBA6O0tDReeeWVuPfee+OWW26Jmpqa6NOnz+6eKtDECQ3YwyxevDi+/vWvR58+fWLmzJnRs2fPov0//OEP40c/+lGUlm77Ac8NGzZE27Ztc08VaAY8dQJ7mGnTpsWGDRvi9ttvrxcZEREtW7aMiy66KHr16hUREWPHjo299torFi9eHKeeemq0a9cuRo8eHRERjz/+ePz1X/919O7dO8rLy6NXr17x93//9/HBBx8UXWfddbz22msxbNiwaNu2bVRVVcVVV10V2/oA6R//+MfRr1+/KC8vj6OPPjqeeeaZXfyTABqCRzRgD/PAAw/EfvvtF9XV1Tt8mc2bN8ewYcPi+OOPj2uuuSYqKioiIuKee+6J999/P8aNGxedO3eOuXPnxo033hh/+MMf4p577im6ji1btsQpp5wSX/jCF2LatGkxffr0mDx5cmzevDmuuuqqorF33nlnrFu3Ls4///woKSmJadOmxRlnnBGvvfZatGrV6s//IQANJwF7jDVr1qSISKeffnq9fe+99156++23C1/vv/9+SimlMWPGpIhI//AP/1DvMnVjPm7KlCmppKQkLVmypLCt7jq+9a1vFbbV1tamESNGpLKysvT222+nlFKqqalJEZE6d+6cVq1aVRh7//33p4hIv/rVrz774oHdwlMnsAdZu3ZtRETstdde9fYNHjw4unbtWvi6+eabi/aPGzeu3mXatGlT+PeGDRvinXfeiWOPPTZSSjFv3rx64ydMmFD4d0lJSUyYMCE2btwYM2bMKBo3atSo6NixY+H7E044ISIiXnvttR1ZJtCIeOoE9iDt2rWLiIj169fX23frrbfGunXrYsWKFfG3f/u3RftatmwZe++9d73LLF26NK644or4n//5n3jvvfeK9q1Zs6bo+9LS0ujbt2/Rtv79+0fEH/9+xsf17t276Pu66PjkMYDGT2jAHqR9+/bRs2fPWLBgQb19da/Z+OSdfkREeXl5vXehbNmyJf7iL/4iVq1aFRMnTowDDjgg2rZtG2+++WaMHTs2amtrP/M8W7RosdXtaRsvHAUaL0+dwB5mxIgRsWjRopg7d+6fdT0vvvhi/P73v49rr702Jk6cGCNHjoyhQ4dGVVXVVsfX1tbWe+rj97//fURE7LPPPn/WXIDGS2jAHuayyy6LioqKOOecc2LFihX19u/oowZ1jzp8fHxKKW644YZtXuamm24qGnvTTTdFq1atYsiQITs6faCJ8dQJ7GH233//uPPOO+PMM8+MAQMGFP4yaEopampq4s4774zS0tKtvibj4w444IDo169ffPe7340333wzKisr47//+7+3+TqK1q1bx/Tp02PMmDFRXV0dv/71r+PBBx+M733ve9G1a9ccSwUaAaEBe6CRI0fGiy++GNdee2385je/idtuuy1KSkqiT58+MWLEiLjgggti4MCBn3odrVq1il/96ldx0UUXxZQpU6J169bxla98JSZMmLDVy7Zo0SKmT58e48aNi0svvTTatWsXkydPjiuuuCLXMoFGoCR5dRWQ2dixY+O//uu/tvpuF6B58xoNACAboQEAZCM0AIBsvEYDAMjGIxoAQDZCAwDIpsH/jkZtbW289dZb0a5duygpKWnowwMAn0FKKdatWxdVVVX1Pvvo0zR4aLz11lvRq1evhj4sALALvPHGG9v9y8Ef1+ChUfcx1W+88UZUVlY29OEBgM9g7dq10atXr8L9+I5q8NCoe7qksrJSaABAE7OzL3vwYlAAIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDYN/jHxdQ6Z/HCUllfsrsMD0Iy9PnXE7p4C/59HNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIZqdD47HHHovTTjstqqqqoqSkJO67774c8wIAmoGdDo0NGzbEwIED4+abb84xHwCgGWm5sxcYPnx4DB8+PMdcAIBmxms0AIBsdvoRjZ310UcfxUcffVT4fu3atbkPCQA0Etkf0ZgyZUq0b9++8NWrV6/chwQAGonsoTFp0qRYs2ZN4euNN97IfUgAoJHI/tRJeXl5lJeX5z4MANAI7XRorF+/PhYtWlT4vqamJubPnx+dOnWK3r1779LJAQBN206HxrPPPhtf/OIXC99ffPHFERExZsyYuOOOO3bZxACApm+nQ2Pw4MGRUsoxFwCgmfF3NACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDYtd9eBF3x/WFRWVu6uwwMADcAjGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGTTcncd+JDJD0dpecXuOjwANDuvTx2xu6dQj0c0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMjmM4XGzTffHPvss0+0bt06qqurY+7cubt6XgBAM7DToXH33XfHxRdfHJMnT47/+7//i4EDB8awYcNi5cqVOeYHADRhOx0a1113XZx77rlx9tlnx0EHHRT/9m//FhUVFXHbbbflmB8A0ITtVGhs3LgxnnvuuRg6dOifrqC0NIYOHRpz5szZ6mU++uijWLt2bdEXALBn2KnQeOedd2LLli3RvXv3ou3du3eP5cuXb/UyU6ZMifbt2xe+evXq9dlnCwA0KdnfdTJp0qRYs2ZN4euNN97IfUgAoJFouTODu3TpEi1atIgVK1YUbV+xYkX06NFjq5cpLy+P8vLyzz5DAKDJ2qlHNMrKyuLII4+MmTNnFrbV1tbGzJkzY9CgQbt8cgBA07ZTj2hERFx88cUxZsyYOOqoo+KYY46Jf/mXf4kNGzbE2WefnWN+AEATttOhMWrUqHj77bfjiiuuiOXLl8fnP//5mD59er0XiAIA7HRoRERMmDAhJkyYsKvnAgA0Mz7rBADIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEaAEA2QgMAyEZoAADZCA0AIBuhAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZNNydx14wfeHRWVl5e46PADQADyiAQBkIzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDZCAwDIRmgAANm0bOgDppQiImLt2rUNfWgA4DOqu9+uux/fUQ0eGu+++25ERPTq1auhDw0A/JnWrVsX7du33+HxDR4anTp1ioiIpUuX7tREm4K1a9dGr1694o033ojKysrdPZ1drjmvz9qarua8vua8tojmvb7muLaUUqxbty6qqqp26nINHhqlpX98WUj79u2bzQ//kyorK5vt2iKa9/qsrelqzutrzmuLaN7ra25r+ywPEHgxKACQjdAAALJpceWVV17Z4Adt0SIGDx4cLVs2+DM32TXntUU07/VZW9PVnNfXnNcW0bzX15zXtjNK0s6+TwUAYAd56gQAyEZoAADZCA0AIBuhAQBk06ChcfPNN8c+++wTrVu3jurq6pg7d25DHn6HTJkyJY4++uho165ddOvWLU4//fRYuHBh0ZjBgwdHSUlJ0dcFF1xQNGbp0qUxYsSIqKioiG7dusWll14amzdvLhoze/bsOOKII6K8vDz222+/uOOOO7Ku7corr6w37wMOOKCw/8MPP4zx48dH586dY6+99oqvfvWrsWLFika/rjr77LNPvfWVlJTE+PHjI6JpnbfHHnssTjvttKiqqoqSkpK47777ivanlOKKK66Inj17Rps2bWLo0KHx6quvFo1ZtWpVjB49OiorK6NDhw7xzW9+M9avX1805oUXXogTTjghWrduHb169Ypp06bVm8s999wTBxxwQLRu3ToOPfTQeOihh7Kub9OmTTFx4sQ49NBDo23btlFVVRVnnXVWvPXWW0XXsbXzPXXq1N2+vu2du7Fjx9ab9ymnnFI0prGeu+2tbWu/fyUlJXH11VcXxjTW87Yjt/0NeRvZFO4vd1hqIHfddVcqKytLt912W/rd736Xzj333NShQ4e0YsWKhprCDhk2bFi6/fbb04IFC9L8+fPTqaeemnr37p3Wr19fGHPSSSelc889Ny1btqzwtWbNmsL+zZs3p0MOOSQNHTo0zZs3Lz300EOpS5cuadKkSYUxr732WqqoqEgXX3xxeumll9KNN96YWrRokaZPn55tbZMnT04HH3xw0bzffvvtwv4LLrgg9erVK82cOTM9++yz6Qtf+EI69thjG/266qxcubJobY888kiKiDRr1qyUUtM6bw899FD6x3/8x3TvvfemiEi//OUvi/ZPnTo1tW/fPt13333p+eefT1/+8pfTvvvumz744IPCmFNOOSUNHDgwPfXUU+nxxx9P++23XzrzzDML+9esWZO6d++eRo8enRYsWJB+/vOfpzZt2qRbb721MOaJJ55ILVq0SNOmTUsvvfRS+qd/+qfUqlWr9OKLL2Zb3+rVq9PQoUPT3XffnV555ZU0Z86cdMwxx6Qjjzyy6Dr69OmTrrrqqqLz+fHf0921vu2duzFjxqRTTjmlaN6rVq0qGtNYz9321vbxNS1btizddtttqaSkJC1evLgwprGetx257W+o28imcn+5oxosNI455pg0fvz4wvdbtmxJVVVVacqUKQ01hc9k5cqVKSLS//7v/xa2nXTSSenb3/72Ni/z0EMPpdLS0rR8+fLCtltuuSVVVlamjz76KKWU0mWXXZYOPvjgosuNGjUqDRs2bBev4E8mT56cBg4cuNV9q1evTq1atUr33HNPYdvLL7+cIiLNmTMnpdR417Ut3/72t1O/fv1SbW1tSqnpnrdP3qDX1tamHj16pKuvvrqwbfXq1am8vDz9/Oc/Tyml9NJLL6WISM8880xhzK9//etUUlKS3nzzzZRSSj/60Y9Sx44dC2tLKaWJEyemAQMGFL7/2te+lkaMGFE0n+rq6nT++ednW9/WzJ07N0VEWrJkSWFbnz590vXXX7/NyzSG9W0rNEaOHLnNyzSVc7cj523kyJHp5JNPLtrWFM5bSvVv+xvyNrKp3l9uS4M8dbJx48Z47rnnYujQoYVtpaWlMXTo0JgzZ05DTOEzW7NmTUT86cPg6vznf/5ndOnSJQ455JCYNGlSvP/++4V9c+bMiUMPPTS6d+9e2DZs2LBYu3Zt/O53vyuM+fjPo25M7p/Hq6++GlVVVdG3b98YPXp0LF26NCIinnvuudi0aVPRnA444IDo3bt3YU6NeV2ftHHjxvjZz34W55xzTpSUlBS2N9Xz9nE1NTWxfPnyonm0b98+qquri85Vhw4d4qijjiqMGTp0aJSWlsbTTz9dGHPiiSdGWVlZYcywYcNi4cKF8d577xXG7O71Rvzx97CkpCQ6dOhQtH3q1KnRuXPnOPzww+Pqq68ueoi6Ma9v9uzZ0a1btxgwYECMGzeu8KnWdXNqDuduxYoV8eCDD8Y3v/nNevuawnn75G1/Q91GNuX7y21pkD9X9s4778SWLVuKfvgREd27d49XXnmlIabwmdTW1sZ3vvOdOO644+KQQw4pbP+bv/mb6NOnT1RVVcULL7wQEydOjIULF8a9994bERHLly/f6lrr9n3amLVr18YHH3wQbdq02eXrqa6ujjvuuCMGDBgQy5Yti+9///txwgknxIIFC2L58uVRVlZW74a8e/fu253z7l7X1tx3332xevXqGDt2bGFbUz1vn1Q3l63N4+Pz7NatW9H+li1bRqdOnYrG7LvvvvWuo25fx44dt7neuutoCB9++GFMnDgxzjzzzKIPp7roooviiCOOiE6dOsWTTz4ZkyZNimXLlsV1111XWENjXN8pp5wSZ5xxRuy7776xePHi+N73vhfDhw+POXPmRIsWLZrNufuP//iPaNeuXZxxxhlF25vCedvabX9D3Ua+9957TfL+8tPs2X8XdTvGjx8fCxYsiN/+9rdF288777zCvw899NDo2bNnDBkyJBYvXhz9+vVr6GnusOHDhxf+fdhhh0V1dXX06dMnfvGLXzRYADSUn/zkJzF8+PCijzNuqudtT7Zp06b42te+FimluOWWW4r2XXzxxYV/H3bYYVFWVhbnn39+TJkyJcrLyxt6qjvs61//euHfhx56aBx22GHRr1+/mD17dgwZMmQ3zmzXuu2222L06NHRunXrou1N4bxt67afz6ZBnjrp0qVLtGjRot6rc1esWBE9evRoiCnstAkTJsQDDzwQs2bNir333vtTx1ZXV0dExKJFiyIiokePHltda92+TxtTWVnZYHf6HTp0iP79+8eiRYuiR48esXHjxli9enW9OW1vznX7Pm1MQ65ryZIlMWPGjPi7v/u7Tx3XVM9b3Vw+7fepR48esXLlyqL9mzdvjlWrVu2S89kQv7d1kbFkyZJ45JFHtvtR29XV1bF58+Z4/fXXI6Lxr69O3759o0uXLkX/D5v6uXv88cdj4cKF2/0djGh8521bt/0NdRvZFO8vt6dBQqOsrCyOPPLImDlzZmFbbW1tzJw5MwYNGtQQU9hhKaWYMGFC/PKXv4xHH3203kN4WzN//vyIiOjZs2dERAwaNChefPHFohuLuhvKgw46qDDm4z+PujEN+fNYv359LF68OHr27BlHHnlktGrVqmhOCxcujKVLlxbm1FTWdfvtt0e3bt1ixIgRnzquqZ63fffdN3r06FE0j7Vr18bTTz9ddK5Wr14dzz33XGHMo48+GrW1tYXAGjRoUDz22GOxadOmwphHHnkkBgwYEB07diyM2R3rrYuMV199NWbMmBGdO3fe7mXmz58fpaWlhacdGvP6Pu4Pf/hDvPvuu0X/D5vyuYv44yOKRx55ZAwcOHC7YxvLedvebX9D3UY2pfvLHdZQrzq96667Unl5ebrjjjvSSy+9lM4777zUoUOHolfnNgbjxo1L7du3T7Nnzy56+9X777+fUkpp0aJF6aqrrkrPPvtsqqmpSffff3/q27dvOvHEEwvXUfcWpy996Utp/vz5afr06alr165bfYvTpZdeml5++eV08803Z38b6CWXXJJmz56dampq0hNPPJGGDh2aunTpklauXJlS+uNbt3r37p0effTR9Oyzz6ZBgwalQYMGNfp1fdyWLVtS796908SJE4u2N7Xztm7dujRv3rw0b968FBHpuuuuS/PmzSu862Lq1KmpQ4cO6f77708vvPBCGjly5Fbf3nr44Yenp59+Ov32t79N+++/f9FbJFevXp26d++evvGNb6QFCxaku+66K1VUVNR7G2HLli3TNddck15++eU0efLkXfL21k9b38aNG9OXv/zltPfee6f58+cX/R7WvXL/ySefTNdff32aP39+Wrx4cfrZz36Wunbtms4666zdvr5PW9u6devSd7/73TRnzpxUU1OTZsyYkY444oi0//77pw8//LBwHY313G3v/2VKf3x7akVFRbrlllvqXb4xn7ft3fan1HC3kU3l/nJHNVhopJTSjTfemHr37p3KysrSMccck5566qmGPPwOiYitft1+++0ppZSWLl2aTjzxxNSpU6dUXl6e9ttvv3TppZcW/T2GlFJ6/fXX0/Dhw1ObNm1Sly5d0iWXXJI2bdpUNGbWrFnp85//fCorK0t9+/YtHCOXUaNGpZ49e6aysrL0uc99Lo0aNSotWrSosP+DDz5IF154YerYsWOqqKhIX/nKV9KyZcsa/bo+7uGHH04RkRYuXFi0vamdt1mzZm31/+GYMWNSSn98i+vll1+eunfvnsrLy9OQIUPqrfndd99NZ555Ztprr71SZWVlOvvss9O6deuKxjz//PPp+OOPT+Xl5elzn/tcmjp1ar25/OIXv0j9+/dPZWVl6eCDD04PPvhg1vXV1NRs8/ew7m+iPPfcc6m6ujq1b98+tW7dOh144IHpBz/4QdGd9e5a36et7f33309f+tKXUteuXVOrVq1Snz590rnnnlvvDqSxnrvt/b9MKaVbb701tWnTJq1evbre5RvzedvebX9KDXsb2RTuL3eUj4kHALLxWScAQDZCAwDIRmgAANkIDQAgG6EBAGQjNACAbIQGAJCN0AAAshEawG7x+uuvR0lJSeEzZ4DmSWgAANkIDdhD1dbWxrRp02K//faL8vLy6N27d/zzP/9zRES8+OKLcfLJJ0ebNm2ic+fOcd5558X69esLlx08eHB85zvfKbq+008/PcaOHVv4fp999okf/OAHcc4550S7du2id+/e8eMf/7iwv+7TMQ8//PAoKSmJwYMHR0TE7Nmz45hjjom2bdtGhw4d4rjjjoslS5Zk+ikAuQkN2ENNmjQppk6dGpdffnm89NJLceedd0b37t1jw4YNMWzYsOjYsWM888wzcc8998SMGTNiwoQJO32Ma6+9No466qiYN29eXHjhhTFu3LhYuHBhRETMnTs3IiJmzJgRy5Yti3vvvTc2b94cp59+epx00knxwgsvxJw5c+K8886LkpKSXbp2oOG03N0TABreunXr4oYbboibbropxowZExER/fr1i+OPPz7+/d//PT788MP46U9/Gm3bto2IiJtuuilOO+20+OEPfxjdu3ff4eOceuqpceGFF0ZExMSJE+P666+PWbNmxYABA6Jr164REdG5c+fo0aNHRESsWrUq1qxZE3/5l38Z/fr1i4iIAw88cJetG2h4HtGAPdDLL78cH330UQwZMmSr+wYOHFiIjIiI4447LmprawuPRuyoww47rPDvkpKS6NGjR6xcuXKb4zt16hRjx46NYcOGxWmnnRY33HBDLFu2bKeOCTQuQgP2QG3atPmzLl9aWhoppaJtmzZtqjeuVatWRd+XlJREbW3tp1737bffHnPmzIljjz027r777ujfv3889dRTf9Z8gd1HaMAeaP/99482bdrEzJkz6+078MAD4/nnn48NGzYUtj3xxBNRWloaAwYMiIiIrl03vO/LAAABj0lEQVS7Fj3SsGXLlliwYMFOzaGsrKxw2U86/PDDY9KkSfHkk0/GIYccEnfeeedOXTfQeAgN2AO1bt06Jk6cGJdddln89Kc/jcWLF8dTTz0VP/nJT2L06NHRunXrGDNmTCxYsCBmzZoV3/rWt+Ib3/hG4fUZJ598cjz44IPx4IMPxiuvvBLjxo2L1atX79QcunXrFm3atInp06fHihUrYs2aNVFTUxOTJk2KOXPmxJIlS+I3v/lNvPrqq16nAU2Y0IA91OWXXx6XXHJJXHHFFXHggQfGqFGjYuXKlVFRUREPP/xwrFq1Ko4++uj4q7/6qxgyZEjcdNNNhcuec845MWbMmDjrrLPipJNOir59+8YXv/jFnTp+y5Yt41//9V/j1ltvjaqqqhg5cmRUVFTEK6+8El/96lejf//+cd5558X48ePj/PPP39XLBxpISfrkE60AALuIRzQAgGyEBgCQjdAAALIRGgBANkIDAMhGaAAA2QgNACAboQEAZCM0AIBshAYAkI3QAACyERoAQDb/D7ViuWMB8kOfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "objects = counts['labels']\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = counts['count']\n",
    " \n",
    "plt.barh(y_pos, performance, align='center', alpha=1)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('counts')\n",
    "plt.title('Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i hope youre lurking rn. i want to listen to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>05 really taught me a valuable lesson I'm neve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Never had a voice to protest, so you fed me sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Rest in peace &amp; love to you and your family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>100 days until Christmas! soon ready yet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                              tweet\n",
       "0      0  i hope youre lurking rn. i want to listen to h...\n",
       "1      0  05 really taught me a valuable lesson I'm neve...\n",
       "2      0  Never had a voice to protest, so you fed me sh...\n",
       "3      0        Rest in peace & love to you and your family\n",
       "4      0           100 days until Christmas! soon ready yet"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df['tweet'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df)) < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count labels\n",
      "0  17014      0\n",
      "1  14789      1\n"
     ]
    }
   ],
   "source": [
    "count = df_train['labels'].value_counts()\n",
    "counts = pd.DataFrame({'labels':count.index, 'count':count.values})\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count labels\n",
      "0   4277      0\n",
      "1   3697      1\n"
     ]
    }
   ],
   "source": [
    "count = df_test['labels'].value_counts()\n",
    "counts = pd.DataFrame({'labels':count.index, 'count':count.values})\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False,sublinear_tf=True)),\n",
    "                     ('clf', MultinomialNB(alpha=0.31,fit_prior=False)),])\n",
    "text_clf = text_clf.fit(df_train.tweet, df_train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.37%\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(df_test.tweet)\n",
    "text_clf_accuracy = metrics.accuracy_score(df_test.labels,predicted)\n",
    "print(\"Accuracy: %.2f%%\" % (text_clf_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3320  957]\n",
      " [ 927 2770]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_conf_mat = confusion_matrix(df_test.labels, predicted)\n",
    "print(text_clf_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__alpha': (1e-2, 1e-3,3e-1),\n",
    "                'clf__analyzer': ('word','char'),\n",
    "                'clf__lowercase':(True,False),\n",
    "                'clf__fit_prior':(True,False),\n",
    "                'tfidf__sublinear_tf' : (True,False),\n",
    "                'tfidf__smooth_idf':(True,False),\n",
    "                }\n",
    "param = {'vect__analyzer': ('word','char'),\n",
    "                'vect__lowercase':(True,False),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__alpha': (1e-2, 1e-3,3e-1),\n",
    "                'clf__analyzer': ('word','char'),\n",
    "                'clf__lowercase':(True,False),\n",
    "                'clf__fit_prior':(True,False),\n",
    "                'tfidf__sublinear_tf' : (True,False),\n",
    "                'tfidf__smooth_idf':(True,False),\n",
    "                }\n",
    "gs_clf = GridSearchCV(text_clf, param, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(df_train.tweet, df_train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__analyzer': 'word', 'vect__lowercase': True}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.tweet\n",
    "y_train = df_train.labels\n",
    "x_test = df_test.tweet\n",
    "y_test = df_test.labels\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.53%\n"
     ]
    }
   ],
   "source": [
    "NB.fit(x_train_dtm,y_train)\n",
    "NB_BOW_y_predict = NB.predict(x_test_dtm)\n",
    "NB_BOW_accuracy = metrics.accuracy_score(y_test,NB_BOW_y_predict)\n",
    "print(\"Accuracy: %.2f%%\" % (NB_BOW_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3034 1175]\n",
      " [ 893 2759]]\n"
     ]
    }
   ],
   "source": [
    "NB_BOW_conf_mat = confusion_matrix(y_test, NB_BOW_y_predict)\n",
    "print(NB_BOW_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score,precision, recall, average = macro\n",
      "0.7298957054465194\n",
      "0.7296970552952721\n",
      "0.7303324363133377\n",
      "f1_score,precision, recall, average = micro\n",
      "0.7306860124729542\n",
      "0.7306860124729541\n",
      "0.7306860124729541\n",
      "f1_score,precision, recall, average = weighted\n",
      "0.7308719670674092\n",
      "0.7312946015053737\n",
      "0.7306860124729541\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score,precision, recall, average = macro\")\n",
    "print(f1_score(y_test, NB_BOW_y_predict, average=\"macro\"))\n",
    "print(precision_score(y_test, NB_BOW_y_predict, average=\"macro\"))\n",
    "print(recall_score(y_test, NB_BOW_y_predict, average=\"macro\")) \n",
    "print(\"f1_score,precision, recall, average = micro\")\n",
    "print(f1_score(y_test, NB_BOW_y_predict, average=\"micro\"))\n",
    "print(precision_score(y_test, NB_BOW_y_predict, average=\"micro\"))\n",
    "print(recall_score(y_test, NB_BOW_y_predict, average=\"micro\")) \n",
    "print(\"f1_score,precision, recall, average = weighted\")\n",
    "NB_BOW_f1_score = f1_score(y_test, NB_BOW_y_predict, average=\"weighted\")\n",
    "NB_BOW_precision_score = precision_score(y_test, NB_BOW_y_predict, average=\"weighted\")\n",
    "NB_BOW_recall_score = recall_score(y_test, NB_BOW_y_predict, average=\"weighted\")\n",
    "print(NB_BOW_f1_score)\n",
    "print(NB_BOW_precision_score)\n",
    "print(NB_BOW_recall_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),stop_words='english',lowercase=True,analyzer='word')),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=True)),\n",
    "                     ('clf', RandomForestClassifier(max_depth=None, min_samples_leaf=2, n_estimators=64)),])\n",
    "text_clf = text_clf.fit(df_train.tweet, df_train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.87%\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(df_test.tweet)\n",
    "text_clf_accuracy = metrics.accuracy_score(df_test.labels,predicted)\n",
    "print(\"Accuracy: %.2f%%\" % (text_clf_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__n_estimators': [16,32,54,64],\n",
    "                'clf__min_samples_split': [2,3,4],\n",
    "                'clf__max_depth':[10,20,30],\n",
    "                'clf__criterion':('gini','entropy'),\n",
    "                'tfidf__sublinear_tf' : (True,False),\n",
    "                'tfidf__smooth_idf':(True,False),\n",
    "              'vect__analyzer': ('word','char'),\n",
    "                'vect__lowercase':(True,False),\n",
    "                }\n",
    "params = {'tfidf__use_idf': (True, False),\n",
    "           'tfidf__sublinear_tf' : (True,False),\n",
    "          'tfidf__smooth_idf':(True,False),\n",
    "         }\n",
    "gs_clf = GridSearchCV(text_clf, params, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(df_train.tweet, df_train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': False}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.86%\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=30, min_samples_leaf=1, n_estimators=54)\n",
    "rf.fit(x_train_dtm,y_train)\n",
    "rf_BOW_predict = rf.predict(x_test_dtm)\n",
    "rf_BOW_accuracy = metrics.accuracy_score(y_test,rf_BOW_predict)\n",
    "print(\"Accuracy: %.2f%%\" % (rf_BOW_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3911  248]\n",
      " [2421 1238]]\n"
     ]
    }
   ],
   "source": [
    "rf_BOW_conf_mat = confusion_matrix(y_test, rf_BOW_predict)\n",
    "print(rf_BOW_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score,precision,recall, average = macro\n",
      "0.6134176927439958\n",
      "0.7253826831008129\n",
      "0.6393570455508593\n",
      "f1_score,precision,recall, average = micro\n",
      "0.6586083397288309\n",
      "0.6586083397288309\n",
      "0.6586083397288309\n",
      "f1_score,precision,recall, average = weighted\n",
      "0.6218708627748293\n",
      "0.7184930480025894\n",
      "0.6586083397288309\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score,precision,recall, average = macro\")\n",
    "print(f1_score(y_test, rf_BOW_predict, average=\"macro\"))\n",
    "print(precision_score(y_test, rf_BOW_predict, average=\"macro\"))\n",
    "print(recall_score(y_test, rf_BOW_predict, average=\"macro\")) \n",
    "print(\"f1_score,precision,recall, average = micro\")\n",
    "print(f1_score(y_test, rf_BOW_predict, average=\"micro\"))\n",
    "print(precision_score(y_test, rf_BOW_predict, average=\"micro\"))\n",
    "print(recall_score(y_test, rf_BOW_predict, average=\"micro\")) \n",
    "print(\"f1_score,precision,recall, average = weighted\")\n",
    "rf_BOW_f1_score = f1_score(y_test, rf_BOW_predict, average=\"weighted\")\n",
    "rf_BOW_precission = precision_score(y_test, rf_BOW_predict, average=\"weighted\")\n",
    "rf_BOW_recall_score = recall_score(y_test, rf_BOW_predict, average=\"weighted\")\n",
    "print(f1_score(y_test, rf_BOW_predict, average=\"weighted\"))\n",
    "print(precision_score(y_test, rf_BOW_predict, average=\"weighted\"))\n",
    "print(recall_score(y_test, rf_BOW_predict, average=\"weighted\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),stop_words='english',lowercase=True,analyzer='word')),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=True)),\n",
    "                     ('clf', XGBClassifier(learning_rate =0.1, n_jobs=-1,\n",
    "n_estimators=1000,\n",
    "max_depth=200,\n",
    "min_child_weight=1,\n",
    "gamma=0,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'binary:logistic',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27)),])\n",
    "text_clf = text_clf.fit(df_test.tweet, df_test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.18%\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(df_test.tweet)\n",
    "text_clf_accuracy = metrics.accuracy_score(df_test.labels,predicted)\n",
    "print(\"Accuracy: %.2f%%\" % (text_clf_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4261,   16],\n",
       "       [  49, 3648]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_test.labels,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__n_estimators': [16,32,54,64],\n",
    "                'clf__min_samples_split': [2,3,4],\n",
    "                'clf__max_depth':[10,20,30],\n",
    "                'clf__criterion':('gini','entropy'),\n",
    "                'tfidf__sublinear_tf' : (True,False),\n",
    "                'tfidf__smooth_idf':(True,False),\n",
    "              'vect__analyzer': ('word','char'),\n",
    "                'vect__lowercase':(True,False),\n",
    "                }\n",
    "params = {'clf__max_depth':[10,20,30,100,200,1000],\n",
    "         }\n",
    "gs_clf = GridSearchCV(text_clf, params, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(df_train.tweet, df_train.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(learning_rate =0.1,\n",
    "n_estimators=1000,\n",
    "max_depth=10,\n",
    "min_child_weight=1,\n",
    "gamma=0,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'binary:logistic',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27)\n",
    "model.fit(x_train_dtm, y_train)\n",
    "# make predictions for test data\n",
    "XGB_BOW_y_pred = model.predict(x_test_dtm)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "XGB_BOW_accuracy = accuracy_score(y_test, XGB_BOW_y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (XGB_BOW_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3407  752]\n",
      " [ 901 2758]]\n"
     ]
    }
   ],
   "source": [
    "XGB_BOW_conf_mat = confusion_matrix(y_test, XGB_BOW_y_pred)\n",
    "print(XGB_BOW_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score,precision,recall, average = macro\n",
      "0.7870976870926178\n",
      "0.7883046052266107\n",
      "0.7864725809893045\n",
      "f1_score,precision,recall, average = micro\n",
      "0.7885648503453568\n",
      "0.7885648503453568\n",
      "0.7885648503453568\n",
      "f1_score,precision,recall, average = weighted\n",
      "0.7882280131733105\n",
      "0.7885648503453568\n",
      "0.7884676660779555\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score,precision,recall, average = macro\")\n",
    "print(f1_score(y_test, XGB_BOW_y_pred, average=\"macro\"))\n",
    "print(precision_score(y_test, XGB_BOW_y_pred, average=\"macro\"))\n",
    "print(recall_score(y_test, XGB_BOW_y_pred, average=\"macro\")) \n",
    "print(\"f1_score,precision,recall, average = micro\")\n",
    "print(f1_score(y_test, XGB_BOW_y_pred, average=\"micro\"))\n",
    "print(precision_score(y_test, XGB_BOW_y_pred, average=\"micro\"))\n",
    "print(recall_score(y_test, XGB_BOW_y_pred, average=\"micro\")) \n",
    "print(\"f1_score,precision,recall, average = weighted\")\n",
    "XGB_BOW_f1_score = f1_score(y_test, XGB_BOW_y_pred, average=\"weighted\")\n",
    "XGB_BOW_recall_score = recall_score(y_test, XGB_BOW_y_pred, average=\"weighted\")\n",
    "XGB_BOW_precision_score = precision_score(y_test, XGB_BOW_y_pred, average=\"weighted\")\n",
    "print(f1_score(y_test, XGB_BOW_y_pred, average=\"weighted\"))\n",
    "print(recall_score(y_test, XGB_BOW_y_pred, average=\"weighted\")) \n",
    "print(precision_score(y_test, XGB_BOW_y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import multiprocessing\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import classification_report\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "    scores = ['precision', 'recall']\n",
    "\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score,n_jobs=-1,pre_dispatch='2*n_jobs')\n",
    "        clf.fit(x_train_dtm, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        y_true, y_pred = y_test, clf.predict(x_test_dtm)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = svm.SVC(kernel ='rbf', C = 100, gamma= 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.fit(x_train_dtm,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_BOW_Predict = model_.predict(x_test_dtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.20%\n"
     ]
    }
   ],
   "source": [
    "svm_BOW_accuracy = metrics.accuracy_score(y_test,svm_BOW_Predict)\n",
    "print(\"Accuracy: %.2f%%\" % (svm_BOW_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4159    0]\n",
      " [3659    0]]\n"
     ]
    }
   ],
   "source": [
    "svm_BOW_conf_mat = confusion_matrix(y_test, svm_BOW_Predict)\n",
    "print(svm_BOW_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score,precision,recall, average = macro\n",
      "0.3472488937129498\n",
      "0.2659887439242773\n",
      "0.5\n",
      "f1_score,precision,recall, average = micro\n",
      "0.5319774878485546\n",
      "0.5319774878485546\n",
      "0.5319774878485546\n",
      "f1_score,precision,recall, average = weighted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3694571882712096\n",
      "0.5319774878485546\n",
      "0.283000047577659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score,precision,recall, average = macro\")\n",
    "print(f1_score(y_test, svm_BOW_Predict, average=\"macro\"))\n",
    "print(precision_score(y_test, svm_BOW_Predict, average=\"macro\"))\n",
    "print(recall_score(y_test, svm_BOW_Predict, average=\"macro\")) \n",
    "print(\"f1_score,precision,recall, average = micro\")\n",
    "print(f1_score(y_test, svm_BOW_Predict, average=\"micro\"))\n",
    "print(precision_score(y_test, svm_BOW_Predict, average=\"micro\"))\n",
    "print(recall_score(y_test, svm_BOW_Predict, average=\"micro\")) \n",
    "print(\"f1_score,precision,recall, average = weighted\")\n",
    "SVM_BOW_f1_score = f1_score(y_test, svm_BOW_Predict, average=\"weighted\")\n",
    "SVM_BOW_recall_score = recall_score(y_test, svm_BOW_Predict, average=\"weighted\")\n",
    "SVM_BOW_precision_score = precision_score(y_test, svm_BOW_Predict, average=\"weighted\")\n",
    "print(f1_score(y_test, svm_BOW_Predict, average=\"weighted\"))\n",
    "print(recall_score(y_test, svm_BOW_Predict, average=\"weighted\")) \n",
    "print(precision_score(y_test, svm_BOW_Predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive bayes</td>\n",
       "      <td>0.740599</td>\n",
       "      <td>0.741502</td>\n",
       "      <td>0.740599</td>\n",
       "      <td>0.740813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.658608</td>\n",
       "      <td>0.718493</td>\n",
       "      <td>0.658608</td>\n",
       "      <td>0.621871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.531977</td>\n",
       "      <td>0.283000</td>\n",
       "      <td>0.531977</td>\n",
       "      <td>0.369457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.788565</td>\n",
       "      <td>0.788468</td>\n",
       "      <td>0.788565</td>\n",
       "      <td>0.788228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision    Recall  F1_Weighted\n",
       "0    Naive bayes  0.740599   0.741502  0.740599     0.740813\n",
       "1  Random Forest  0.658608   0.718493  0.658608     0.621871\n",
       "2            SVM  0.531977   0.283000  0.531977     0.369457\n",
       "3            XGB  0.788565   0.788468  0.788565     0.788228"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu = pd.DataFrame({\"Model\" : [\"Naive bayes\",\"Random Forest\",\"SVM\",\"XGB\"],\n",
    "                     \"Accuracy\" : [NB_BOW_accuracy,rf_BOW_accuracy,svm_BOW_accuracy,XGB_BOW_accuracy],\n",
    "                     \"Precision\" :[NB_BOW_precision_score,rf_BOW_precission,SVM_BOW_precision_score,XGB_BOW_precision_score],\n",
    "                     \"Recall\" : [NB_BOW_recall_score,rf_BOW_recall_score,SVM_BOW_recall_score,XGB_BOW_recall_score],\n",
    "                     \"F1_Weighted\" : [NB_BOW_f1_score,rf_BOW_f1_score,SVM_BOW_f1_score,XGB_BOW_f1_score]\n",
    "                    })\n",
    "accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# x = vectorizer.fit_transform(data.sent)\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(X.shape)\n",
    "\n",
    "x_train = df_train.tweet\n",
    "y_train = df_train.labels\n",
    "x_test = df_test.tweet\n",
    "y_test = df_test.labels\n",
    "x_train_dtm1 = vectorizer.fit_transform(x_train)\n",
    "x_test_dtm1 = vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2),lowercase=True,analyzer='word')),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=False,smooth_idf=False)),\n",
    "                     ('clf', MultinomialNB(alpha=2.0,fit_prior=False)),])\n",
    "text_clf = text_clf.fit(df_train.tweet, df_train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.81%\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(df_test.tweet)\n",
    "text_clf_accuracy = metrics.accuracy_score(df_test.labels,predicted)\n",
    "print(\"Accuracy: %.2f%%\" % (text_clf_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 2.0}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__alpha': (1e-2, 1e-3,3e-1,2.0),\n",
    "                'vect__analyzer': ('word','char'),\n",
    "                'vect__lowercase':(True,False),\n",
    "                'clf__fit_prior':(True,False),\n",
    "                'tfidf__smooth_idf':(True,False),\n",
    "                }\n",
    "params = {'clf__alpha': (2.0,3.0),\n",
    "         }\n",
    "gs_clf = GridSearchCV(text_clf, params, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(df_train.tweet, df_train.labels)\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.48%\n"
     ]
    }
   ],
   "source": [
    "NB.fit(x_train_dtm1,y_train)\n",
    "NB_TF_predict = NB.predict(x_test_dtm)\n",
    "NB_TF_accuracy = metrics.accuracy_score(y_test,NB_TF_predict)\n",
    "print(\"Accuracy: %.2f%%\" % (NB_TF_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3097 1062]\n",
      " [ 933 2726]]\n"
     ]
    }
   ],
   "source": [
    "NB_TF_conf_mat = confusion_matrix(y_test, NB_TF_predict)\n",
    "print(NB_TF_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score,precision,recall, average = macro\n",
      "0.7442436995604204\n",
      "0.7440636619231162\n",
      "0.7448312273648832\n",
      "f1_score,precision,recall, average = micro\n",
      "0.7448196469685342\n",
      "0.7448196469685342\n",
      "0.7448196469685342\n",
      "f1_score,precision,recall, average = weighted\n",
      "0.7450199090053177\n",
      "0.7448196469685342\n",
      "0.7456256144962938\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score,precision,recall, average = macro\")\n",
    "print(f1_score(y_test, NB_TF_predict, average=\"macro\"))\n",
    "print(precision_score(y_test, NB_TF_predict, average=\"macro\"))\n",
    "print(recall_score(y_test, NB_TF_predict, average=\"macro\")) \n",
    "print(\"f1_score,precision,recall, average = micro\")\n",
    "print(f1_score(y_test, NB_TF_predict, average=\"micro\"))\n",
    "print(precision_score(y_test, NB_TF_predict, average=\"micro\"))\n",
    "print(recall_score(y_test, NB_TF_predict, average=\"micro\")) \n",
    "print(\"f1_score,precision,recall, average = weighted\")\n",
    "NB_TF_f1_score = f1_score(y_test, NB_TF_predict, average=\"weighted\")\n",
    "NB_TF_recall_score = recall_score(y_test, NB_TF_predict, average=\"weighted\")\n",
    "NB_TF_precision_score = precision_score(y_test, NB_TF_predict, average=\"weighted\")\n",
    "print(f1_score(y_test, NB_TF_predict, average=\"weighted\"))\n",
    "print(recall_score(y_test, NB_TF_predict, average=\"weighted\")) \n",
    "print(precision_score(y_test, NB_TF_predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,1),stop_words='english',lowercase=True,analyzer='word')),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=False)),\n",
    "                     ('clf', RandomForestClassifier(max_depth=None, min_samples_leaf=2, n_estimators=1024)),])\n",
    "text_clf = text_clf.fit(df_train.tweet, df_train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.52%\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(df_test.tweet)\n",
    "text_clf_accuracy = metrics.accuracy_score(df_test.labels,predicted)\n",
    "print(\"Accuracy: %.2f%%\" % (text_clf_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': False}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__n_estimators': [16,32,54,64],\n",
    "                'clf__min_samples_split': [2,3,4],\n",
    "                'clf__max_depth':[10,20,30],\n",
    "                'clf__criterion':('gini','entropy'),\n",
    "                'tfidf__sublinear_tf' : (True,False),\n",
    "                'tfidf__smooth_idf':(True,False),\n",
    "              'vect__analyzer': ('word','char'),\n",
    "                'vect__lowercase':(True,False),\n",
    "                }\n",
    "params = {'tfidf__use_idf': (True, False),\n",
    "           'tfidf__sublinear_tf' : (True,False),\n",
    "          'tfidf__smooth_idf':(True,False),\n",
    "         }\n",
    "gs_clf = GridSearchCV(text_clf, params, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(df_train.tweet, df_train.labels)\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.54%\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=30, min_samples_leaf=1, n_estimators=54)\n",
    "rf.fit(x_train_dtm1,y_train)\n",
    "rf_TF_predict = rf.predict(x_test_dtm1)\n",
    "rf_TF_accuracy = metrics.accuracy_score(y_test,rf_TF_predict)\n",
    "print(\"Accuracy: %.2f%%\" % (rf_TF_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3918  241]\n",
      " [2375 1284]]\n"
     ]
    }
   ],
   "source": [
    "rf_TF_conf_mat = confusion_matrix(y_test, rf_TF_predict)\n",
    "print(rf_TF_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score,precision,recall, average = macro\n",
      "0.6225416719819704\n",
      "0.73228187447411\n",
      "0.6464844644564145\n",
      "f1_score,precision,recall, average = micro\n",
      "0.6653875671527245\n",
      "0.6653875671527245\n",
      "0.6653875671527245\n",
      "f1_score,precision,recall, average = weighted\n",
      "0.6306749094859101\n",
      "0.6653875671527245\n",
      "0.7252669513070182\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score,precision,recall, average = macro\")\n",
    "print(f1_score(y_test, rf_TF_predict, average=\"macro\"))\n",
    "print(precision_score(y_test, rf_TF_predict, average=\"macro\"))\n",
    "print(recall_score(y_test, rf_TF_predict, average=\"macro\")) \n",
    "print(\"f1_score,precision,recall, average = micro\")\n",
    "print(f1_score(y_test, rf_TF_predict, average=\"micro\"))\n",
    "print(precision_score(y_test, rf_TF_predict, average=\"micro\"))\n",
    "print(recall_score(y_test, rf_TF_predict, average=\"micro\")) \n",
    "print(\"f1_score,precision,recall, average = weighted\")\n",
    "rf_TF_f1_score = f1_score(y_test, rf_TF_predict, average=\"weighted\")\n",
    "rf_TF_recall_score = recall_score(y_test, rf_TF_predict, average=\"weighted\")\n",
    "rf_TF_precision_score = precision_score(y_test, rf_TF_predict, average=\"weighted\")\n",
    "print(f1_score(y_test, rf_TF_predict, average=\"weighted\"))\n",
    "print(recall_score(y_test, rf_TF_predict, average=\"weighted\")) \n",
    "print(precision_score(y_test, rf_TF_predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.20%\n"
     ]
    }
   ],
   "source": [
    "model_ = svm.SVC()\n",
    "model_.fit(x_train_dtm1,y_train)\n",
    "svm_TF_Predict = model_.predict(x_test_dtm1)\n",
    "svm_TF_accuracy  = metrics.accuracy_score(y_test,svm_TF_Predict)\n",
    "print(\"Accuracy: %.2f%%\" % (svm_TF_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4159    0]\n",
      " [3659    0]]\n"
     ]
    }
   ],
   "source": [
    "svm_TF_conf_mat = confusion_matrix(y_test, svm_TF_Predict)\n",
    "print(svm_TF_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score,precision,recall, average = macro\n",
      "0.3472488937129498\n",
      "0.2659887439242773\n",
      "0.5\n",
      "f1_score,precision,recall, average = micro\n",
      "0.5319774878485546\n",
      "0.5319774878485546\n",
      "0.5319774878485546\n",
      "f1_score,precision,recall, average = weighted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3694571882712096\n",
      "0.5319774878485546\n",
      "0.283000047577659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score,precision,recall, average = macro\")\n",
    "print(f1_score(y_test, svm_TF_Predict, average=\"macro\"))\n",
    "print(precision_score(y_test, svm_TF_Predict, average=\"macro\"))\n",
    "print(recall_score(y_test, svm_TF_Predict, average=\"macro\")) \n",
    "print(\"f1_score,precision,recall, average = micro\")\n",
    "print(f1_score(y_test, svm_TF_Predict, average=\"micro\"))\n",
    "print(precision_score(y_test, svm_TF_Predict, average=\"micro\"))\n",
    "print(recall_score(y_test, svm_TF_Predict, average=\"micro\")) \n",
    "print(\"f1_score,precision,recall, average = weighted\")\n",
    "svm_TF_f1_score = f1_score(y_test, svm_TF_Predict, average=\"weighted\")\n",
    "svm_TF_recall_score = recall_score(y_test, svm_TF_Predict, average=\"weighted\")\n",
    "svm_TF_precision_score = precision_score(y_test, svm_TF_Predict, average=\"weighted\")\n",
    "print(f1_score(y_test, svm_TF_Predict, average=\"weighted\"))\n",
    "print(recall_score(y_test, svm_TF_Predict, average=\"weighted\")) \n",
    "print(precision_score(y_test, svm_TF_Predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=10,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " saaale_pos_weight=1,\n",
    " seed=27)\n",
    "model.fit(x_train_dtm1, y_train)\n",
    "# make predictions for test data\n",
    "xgb_TF_pred = model.predict(x_test_dtm1)\n",
    "#predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "xgb_tf_accuracy = accuracy_score(y_test, xgb_TF_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (xgb_tf_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3387  772]\n",
      " [ 974 2685]]\n"
     ]
    }
   ],
   "source": [
    "xgb_TF_conf_mat = confusion_matrix(y_test, xgb_TF_pred)\n",
    "print(xgb_TF_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score,precision,recall, average = macro\n",
      "0.7748539296487187\n",
      "0.7766708585453533\n",
      "0.7740927537332809\n",
      "f1_score,precision,recall, average = micro\n",
      "0.7766692248656946\n",
      "0.7766692248656946\n",
      "0.7766692248656946\n",
      "f1_score,precision,recall, average = weighted\n",
      "0.7761468749599552\n",
      "0.7766692248656946\n",
      "0.7766699549614713\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score,precision,recall, average = macro\")\n",
    "print(f1_score(y_test, xgb_TF_pred, average=\"macro\"))\n",
    "print(precision_score(y_test, xgb_TF_pred, average=\"macro\"))\n",
    "print(recall_score(y_test, xgb_TF_pred, average=\"macro\")) \n",
    "print(\"f1_score,precision,recall, average = micro\")\n",
    "print(f1_score(y_test, xgb_TF_pred, average=\"micro\"))\n",
    "print(precision_score(y_test, xgb_TF_pred, average=\"micro\"))\n",
    "print(recall_score(y_test, xgb_TF_pred, average=\"micro\")) \n",
    "print(\"f1_score,precision,recall, average = weighted\")\n",
    "xgb_TF_f1_score = f1_score(y_test, xgb_TF_pred, average=\"weighted\")\n",
    "xgb_TF_recall_score = recall_score(y_test, xgb_TF_pred, average=\"weighted\")\n",
    "xgb_TF_precision_score = precision_score(y_test, xgb_TF_pred, average=\"weighted\")\n",
    "print(f1_score(y_test, xgb_TF_pred, average=\"weighted\"))\n",
    "print(recall_score(y_test, xgb_TF_pred, average=\"weighted\")) \n",
    "print(precision_score(y_test, xgb_TF_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive bayes</td>\n",
       "      <td>0.744820</td>\n",
       "      <td>0.745626</td>\n",
       "      <td>0.744820</td>\n",
       "      <td>0.745020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.665388</td>\n",
       "      <td>0.725267</td>\n",
       "      <td>0.665388</td>\n",
       "      <td>0.630675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.531977</td>\n",
       "      <td>0.283000</td>\n",
       "      <td>0.531977</td>\n",
       "      <td>0.369457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.776669</td>\n",
       "      <td>0.776670</td>\n",
       "      <td>0.776669</td>\n",
       "      <td>0.776147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision    Recall  F1_Weighted\n",
       "0    Naive bayes  0.744820   0.745626  0.744820     0.745020\n",
       "1  Random Forest  0.665388   0.725267  0.665388     0.630675\n",
       "2            SVM  0.531977   0.283000  0.531977     0.369457\n",
       "3            XGB  0.776669   0.776670  0.776669     0.776147"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu = pd.DataFrame({\"Model\" : [\"Naive bayes\",\"Random Forest\",\"SVM\",\"XGB\"],\n",
    "                     \"Accuracy\" : [NB_TF_accuracy,rf_TF_accuracy,svm_TF_accuracy,xgb_tf_accuracy],\n",
    "                     \"Precision\" :[NB_TF_precision_score,rf_TF_precision_score,svm_TF_precision_score,xgb_TF_precision_score],\n",
    "                     \"Recall\" : [NB_TF_recall_score,rf_TF_recall_score,svm_TF_recall_score,xgb_TF_recall_score],\n",
    "                     \"F1_Weighted\" : [NB_TF_f1_score,rf_TF_f1_score,svm_TF_f1_score,xgb_TF_f1_score]\n",
    "                    })\n",
    "accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('df_test.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total texts in train: 31920\n",
      "total texts in test: 7857\n"
     ]
    }
   ],
   "source": [
    "print('total texts in train:',len(df_train.tweet))\n",
    "print('total texts in test:',len(df_test.tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  39742\n",
      "length of word2index:  39742\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for text in df_train.tweet:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()]+=1\n",
    "\n",
    "for text in df_test.tweet:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()]+=1\n",
    "\n",
    "total_words = len(vocab)\n",
    "print(\"vocab length: \",(total_words))\n",
    "\n",
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2index[word.lower()] = i\n",
    "\n",
    "    return word2index\n",
    "word2index = get_word_2_index(vocab)\n",
    "print(\"length of word2index: \",len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(df,i,batch_size):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df.tweet[i*batch_size:i*batch_size+batch_size]\n",
    "    categories = df.labels[i*batch_size:i*batch_size+batch_size]\n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float)\n",
    "        for word in text.split(' '):\n",
    "            layer[word2index[word.lower()]] += 1\n",
    "\n",
    "        batches.append(layer)\n",
    "\n",
    "    for category in categories:\n",
    "        index_y = -1\n",
    "        if category == 0:\n",
    "            index_y = 0\n",
    "        else:\n",
    "            index_y = 1\n",
    "        results.append(index_y)\n",
    "\n",
    "\n",
    "    return np.array(batches),np.array(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 200\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "hidden_size = 1000      # 1st layer and 2nd layer number of features\n",
    "input_size = total_words # Words in vocab\n",
    "num_classes = 2         # Categories: graphics, sci.space and baseball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(OurNet, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size,hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OurNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/159], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [100/159], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [100/159], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [100/159], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [100/159], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(df_train.tweet)/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y = get_batch(df_train,i,batch_size)\n",
    "        articles = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        #print(\"articles\",articles)\n",
    "        #print(batch_x, labels)\n",
    "        #print(\"size labels\",labels.size())\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(df_train.tweet)//batch_size, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1180 test articles: 100 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "total_test_data = len(df_test.labels)\n",
    "batch_x_test,batch_y_test = get_batch(df_test,0,total_test_data)\n",
    "articles = Variable(torch.FloatTensor(batch_x_test))\n",
    "labels = torch.LongTensor(batch_y_test)\n",
    "outputs = net(articles)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "total += labels.size(0)\n",
    "correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 1180 test articles: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN_MODEL_YonKim_Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, \n",
    "                       tokenize=tokenizer, \n",
    "                       include_lengths=True, \n",
    "                       use_vocab=True,\n",
    "                       fix_length=30)\n",
    "LABEL = data.Field(sequential=False, \n",
    "                         use_vocab=False, \n",
    "                         pad_token=None, \n",
    "                         unk_token=None,\n",
    "                         dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_fields = [\n",
    "    ('labels', LABEL), # process it as label\n",
    "    ('tweet', TEXT) # process it as text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.TabularDataset.splits(path='/home/ishant/ML/Sarcastic Model/Sarcasm_detection/', \n",
    "                                            format='csv', \n",
    "                                            train='df_train.csv', \n",
    "                                            validation='df_test.csv', \n",
    "                                            fields=train_val_fields, \n",
    "                                            skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22344, 7857, 9576)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test) ,len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('labels', <torchtext.data.field.Field object at 0x7fa33030f160>), ('tweet', <torchtext.data.field.Field object at 0x7fa33030f0f0>)])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.fields.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.data.example.Example"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'photo', 'is', 'not', 'right', 'surprised', 'he', 'didn', 't', 'delete', 'it', 'even', 'if', 'it', 's', 'from', 'san', 'francisco', 'could', 'the', 'black', 'on', 'white', 'rate', 'be', 'that', 'high']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(example.tweet)\n",
    "print(example.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vocab.Vectors('glove.twitter.27B.50d.txt', '/home/ishant/ML/Sarcastic Model/Sarcasm_detection/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=25000, vectors=vec)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TEXT.vocab.vectors[TEXT.vocab.stoi['the']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17568, 50])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "traindl, valdl = data.BucketIterator.splits(datasets=(train,valid), # specify train and validation Tabulardataset\n",
    "                                            batch_sizes=(BATCH_SIZE,BATCH_SIZE),  # batch size of train and validation\n",
    "                                            sort_key=lambda x: len(x.tweet), # on what attribute the text should be sorted\n",
    "                                            device=-1, # -1 mean cpu and 0 or None mean gpu\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "traindl, valdl,testdl = data.BucketIterator.splits(datasets=(train,valid,test), # specify train and validation Tabulardataset\n",
    "                                            batch_sizes=(BATCH_SIZE,BATCH_SIZE,BATCH_SIZE),  # batch size of train and validation\n",
    "                                            sort_key=lambda x: len(x.tweet), # on what attribute the text should be sorted\n",
    "                                            device=-1, # -1 mean cpu and 0 or None mean gpu\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 150\n"
     ]
    }
   ],
   "source": [
    "print(len(traindl), len(valdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.batch.Batch'>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(traindl)) # BucketIterator return a batch object\n",
    "print(type(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 0., 1.])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_field):\n",
    "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_field)\n",
    "            yield (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = BatchGenerator(traindl, 'tweet', 'labels')\n",
    "valid_iterator = BatchGenerator(valdl, 'tweet', 'labels')\n",
    "test_iterator = BatchGenerator(testdl, 'tweet', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iterator)) # BucketIterator return a batch object\n",
    "print(type(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0],embedding_dim))\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1],embedding_dim))\n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2],embedding_dim))\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embedding_dim)) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        #save_x = x\n",
    "        #print(x)\n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 50\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0431,  0.5039,  0.2709,  ..., -0.2238, -0.6603, -0.7065],\n",
       "        ...,\n",
       "        [-0.6835, -0.1286, -0.2167,  ..., -0.9572,  0.3453, -0.2812],\n",
       "        [ 0.4758,  0.7332,  0.7221,  ..., -0.5614,  0.7186, -0.2680],\n",
       "        [-0.5404,  1.1035, -0.1208,  ...,  1.0089, -0.4560, -0.4926]])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    #print(y.numpy(),rounded_preds.shape)\n",
    "    #save_preds.append(rounded_preds.detach().numpy())\n",
    "    #save_y.append(y.detach().numpy())\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc,rounded_preds.detach().numpy(),y.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(type(batch.tweet))\n",
    "        predictions = model(batch.tweet[0]).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.labels)\n",
    "        \n",
    "        acc,__,___ = binary_accuracy(predictions, batch.labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "   \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.tweet[0]).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.labels)\n",
    "            \n",
    "            acc,__,___ = binary_accuracy(predictions, batch.labels)\n",
    "            save_preds.append(__)\n",
    "            save_y.append(___)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Train Loss:0.696 Train Acc:53.603 Val. Loss:0.672 Val. Acc:58.67\n",
      "Epoch:1 Train Loss:0.671 Train Acc:58.277 Val. Loss:0.657 Val. Acc:61.95\n",
      "Epoch:2 Train Loss:0.651 Train Acc:61.647 Val. Loss:0.645 Val. Acc:63.31\n",
      "Epoch:3 Train Loss:0.634 Train Acc:64.411 Val. Loss:0.634 Val. Acc:64.63\n",
      "Epoch:4 Train Loss:0.617 Train Acc:66.567 Val. Loss:0.623 Val. Acc:65.81\n",
      "Epoch:5 Train Loss:0.603 Train Acc:67.723 Val. Loss:0.614 Val. Acc:66.54\n",
      "Epoch:6 Train Loss:0.587 Train Acc:69.228 Val. Loss:0.604 Val. Acc:67.50\n",
      "Epoch:7 Train Loss:0.572 Train Acc:70.643 Val. Loss:0.594 Val. Acc:68.41\n",
      "Epoch:8 Train Loss:0.556 Train Acc:71.915 Val. Loss:0.585 Val. Acc:69.13\n",
      "Epoch:9 Train Loss:0.540 Train Acc:73.009 Val. Loss:0.576 Val. Acc:69.62\n",
      "Epoch:10 Train Loss:0.529 Train Acc:73.799 Val. Loss:0.569 Val. Acc:70.04\n",
      "Epoch:11 Train Loss:0.514 Train Acc:74.964 Val. Loss:0.562 Val. Acc:70.51\n",
      "Epoch:12 Train Loss:0.500 Train Acc:75.786 Val. Loss:0.555 Val. Acc:70.91\n",
      "Epoch:13 Train Loss:0.486 Train Acc:76.696 Val. Loss:0.550 Val. Acc:71.29\n",
      "Epoch:14 Train Loss:0.474 Train Acc:77.549 Val. Loss:0.546 Val. Acc:71.66\n",
      "Epoch:15 Train Loss:0.467 Train Acc:78.018 Val. Loss:0.541 Val. Acc:72.06\n",
      "Epoch:16 Train Loss:0.452 Train Acc:79.000 Val. Loss:0.539 Val. Acc:72.41\n",
      "Epoch:17 Train Loss:0.441 Train Acc:79.705 Val. Loss:0.533 Val. Acc:72.52\n",
      "Epoch:18 Train Loss:0.432 Train Acc:80.567 Val. Loss:0.531 Val. Acc:72.82\n",
      "Epoch:19 Train Loss:0.420 Train Acc:80.960 Val. Loss:0.528 Val. Acc:73.22\n",
      "Epoch:20 Train Loss:0.411 Train Acc:81.513 Val. Loss:0.525 Val. Acc:73.54\n",
      "Epoch:21 Train Loss:0.399 Train Acc:82.196 Val. Loss:0.524 Val. Acc:73.92\n",
      "Epoch:22 Train Loss:0.387 Train Acc:82.884 Val. Loss:0.523 Val. Acc:73.98\n",
      "Epoch:23 Train Loss:0.378 Train Acc:83.420 Val. Loss:0.522 Val. Acc:74.29\n",
      "Epoch:24 Train Loss:0.371 Train Acc:83.902 Val. Loss:0.521 Val. Acc:74.37\n",
      "Epoch:25 Train Loss:0.361 Train Acc:84.246 Val. Loss:0.520 Val. Acc:74.49\n",
      "Epoch:26 Train Loss:0.352 Train Acc:84.911 Val. Loss:0.520 Val. Acc:74.49\n",
      "Epoch:27 Train Loss:0.342 Train Acc:85.312 Val. Loss:0.520 Val. Acc:74.72\n",
      "Epoch:28 Train Loss:0.333 Train Acc:85.692 Val. Loss:0.521 Val. Acc:74.78\n",
      "Epoch:29 Train Loss:0.324 Train Acc:86.424 Val. Loss:0.522 Val. Acc:74.83\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "save_y = []\n",
    "save_preds = []\n",
    "final_accu = 1.0\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, traindl, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valdl, criterion)\n",
    "    #flat_list_y = np.array([item for sublist in save_y for item in sublist])\n",
    "    #flat_list_preds = np.array([item for sublist in save_preds for item in sublist])\n",
    "    #confusion_matrix(flat_list_y,flat_list_preds)\n",
    "    final_accu = (valid_acc*100)\n",
    "    print('Epoch:{}'.format(epoch), 'Train Loss:{:.3f}'.format(train_loss), 'Train Acc:{:.3f}'.format((train_acc*100)), 'Val. Loss:{:.3f}'.format(valid_loss), 'Val. Acc:{:.2f}'.format(valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[115697,  37363],\n",
       "       [ 47269,  86951]])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "flat_list_y = np.array([item for sublist in save_y for item in sublist])\n",
    "flat_list_preds = np.array([item for sublist in save_preds for item in sublist])\n",
    "\n",
    "confusion_matrix(flat_list_y,flat_list_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_f1_score = (f1_score(flat_list_y, flat_list_preds, average=\"weighted\"))\n",
    "cnn_recall_score = (recall_score(flat_list_y, flat_list_preds, average=\"weighted\")) \n",
    "cnn_precision_score =(precision_score(flat_list_y, flat_list_preds, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Weighted</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74.833333</td>\n",
       "      <td>0.704376</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.70504</td>\n",
       "      <td>0.705402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  F1_Weighted Model  Precision    Recall\n",
       "0  74.833333     0.704376   CNN    0.70504  0.705402"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "accu = pd.DataFrame({\"Model\" : [\"CNN\"],\n",
    "                     \"Accuracy\" : [final_accu],\n",
    "                     \"Precision\" :[cnn_precision_score],\n",
    "                     \"Recall\" : [cnn_recall_score],\n",
    "                     \"F1_Weighted\" : [cnn_f1_score]\n",
    "                    })\n",
    "accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, testdl, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test. Loss:0.522 Test. Acc:74.83\n"
     ]
    }
   ],
   "source": [
    "print('Test. Loss:{:.3f}'.format(valid_loss), 'Test. Acc:{:.2f}'.format(valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = F.sigmoid(model(tensor))\n",
    "    ans = prediction.item()\n",
    "    if ans > 0.5:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test.examples[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arrests after sporadic fighting at ballymena v coleraine match at showgrounds two people have been arrested',\n",
       " '0')"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \" \".join(a.tweet)\n",
    "lbl = a.labels\n",
    "sent,lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', '0')"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(sent), lbl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, \n",
    "                       tokenize=tokenizer, \n",
    "                       include_lengths=True, \n",
    "                       use_vocab=True,\n",
    "                       fix_length=30)\n",
    "LABEL = data.Field(sequential=False, \n",
    "                         use_vocab=False, \n",
    "                         pad_token=None, \n",
    "                         unk_token=None,\n",
    "                         dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_fields = [\n",
    "    ('labels', LABEL), # process it as label\n",
    "    ('tweet', TEXT) # process it as text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.TabularDataset.splits(path='/home/ishant/ML/Sarcastic Model/Sarcasm_detection/', \n",
    "                                            format='csv', \n",
    "                                            train='df_train.csv', \n",
    "                                            validation='df_test.csv', \n",
    "                                            fields=train_val_fields, \n",
    "                                            skip_header=True)\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "train, valid = train.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vocab.Vectors('glove.twitter.27B.50d.txt', '/home/ishant/ML/Sarcastic Model/Sarcasm_detection/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=25000, vectors=vec)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "traindl, valdl,testdl = data.BucketIterator.splits(datasets=(train,valid,test), # specify train and validation Tabulardataset\n",
    "                                            batch_sizes=(BATCH_SIZE,BATCH_SIZE,BATCH_SIZE),  # batch size of train and validation\n",
    "                                            sort_key=lambda x: len(x.tweet), # on what attribute the text should be sorted\n",
    "                                            device=-1, # -1 mean cpu and 0 or None mean gpu\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "\t\tsuper(LSTMClassifier, self).__init__()\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\tArguments\n",
    "\t\t---------\n",
    "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "\t\toutput_size : 2 = (pos, neg)\n",
    "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
    "\t\tvocab_size : Size of the vocabulary containing unique words\n",
    "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
    "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\t\tself.embedding_length = embedding_length\n",
    "\t\t\n",
    "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
    "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
    "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size,batch_first=True)\n",
    "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
    "\t\t\n",
    "\tdef forward(self, input_sentence, batch_size=None):\n",
    "\t\n",
    "\t\t\"\"\" \n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
    "\t\tfinal_output.shape = (batch_size, output_size)\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\n",
    "\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
    "\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
    "\t\t#if batch_size is None:\n",
    "\t\t#\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size)) # Initial hidden state of the LSTM\n",
    "\t\t#\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size)) # Initial cell state of the LSTM\n",
    "\t\t#else:\n",
    "\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size))\n",
    "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "\t\tfinal_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
    "\t\t\n",
    "\t\treturn final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "batch_size = 64\n",
    "output_size = 2\n",
    "hidden_size = 256\n",
    "embedding_length = 50\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "lstm_model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (word_embeddings): Embedding(17568, 50)\n",
       "  (lstm): LSTM(50, 256, batch_first=True)\n",
       "  (label): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_iter, epoch):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.cuda()\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        print(idx)\n",
    "        text = batch.tweet[0]\n",
    "        target = batch.labels\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not 64):# One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction = model(text)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print (\"Epoch: \".format(epoch+1), \"Idx:\".format(idx+1), \"Training Loss:.4f \".format(loss.item()), \"Training Accuracy: .2f \".format(acc.item()))\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.tweet[0]\n",
    "            if (text.size()[0] is not 64):\n",
    "                continue\n",
    "            target = batch.labels\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = model(text)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_model(lstm_model, traindl, epoch)\n",
    "    val_loss, val_acc = eval_model(lstm_model, valdl)\n",
    "    \n",
    "    #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
    "    print('Epoch:{}'.format(epoch), 'Train Loss:{:.3f}'.format(train_loss), 'Train Acc:{:.3f}'.format((train_acc*100)), 'Val. Loss:{:.3f}'.format(valid_loss), 'Val. Acc:{:.2f}'.format(valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
